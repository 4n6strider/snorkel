{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Sentiment Analysis LSTM Using Noisy Crowd Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adapted for pandas instead of Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll provide a simple walkthrough of how to use Snorkel to resolve conflicts in a noisy crowdsourced dataset for a sentiment analysis task, and then use these denoised labels to train an LSTM sentiment analysis model which can be applied to new, unseen data to automatically make predictions!\n",
    "\n",
    "1. Creating basic Snorkel objects: `Candidates`, `Contexts`, and `Labels`\n",
    "2. Training the `GenerativeModel` to resolve labeling conflicts\n",
    "3. Training a simple LSTM sentiment analysis model, which can then be used on new, unseen data!\n",
    "\n",
    "Note that this is a simple tutorial meant to give an overview of the mechanics of using Snorkel-- we'll note places where more careful fine-tuning could be done!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Detail: Weather Sentiments in Tweets\n",
    "\n",
    "In this tutorial we focus on the [Weather sentiment](https://www.crowdflower.com/data/weather-sentiment/) task from [Crowdflower](https://www.crowdflower.com/).\n",
    "\n",
    "In this task, contributors were asked to grade the sentiment of a particular tweet relating to the weather. Contributors could choose among the following categories:\n",
    "1. Positive\n",
    "2. Negative\n",
    "3. I can't tell\n",
    "4. Neutral / author is just sharing information\n",
    "5. Tweet not related to weather condition\n",
    "\n",
    "The catch is that 20 contributors graded each tweet. Thus, in many cases contributors assigned conflicting sentiment labels to the same tweet. \n",
    "\n",
    "The task comes with two data files (to be found in the `data` directory of the tutorial:\n",
    "1. [weather-non-agg-DFE.csv](data/weather-non-agg-DFE.csv) contains the raw contributor answers for each of the 1,000 tweets.\n",
    "2. [weather-evaluated-agg-DFE.csv](data/weather-evaluated-agg-DFE.csv) contains gold sentiment labels by trusted workers for each of the 1,000 tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Preprocessing - Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load raw crowdsourcing data\n",
    "raw_crowd_answers = pd.read_csv(\"data/weather-non-agg-DFE.csv\")\n",
    "\n",
    "# Load groundtruth crowdsourcing data\n",
    "gold_crowd_answers = pd.read_csv(\"data/weather-evaluated-agg-DFE.csv\")\n",
    "\n",
    "# # filter out low-confidence answers\n",
    "gold_answers = gold_crowd_answers[['tweet_id', 'sentiment', 'tweet_body']][(gold_crowd_answers.correct_category == 'Yes') & (gold_crowd_answers.correct_category_conf == 1)] \n",
    "\n",
    "# # keep only the tweets with available groundtruth\n",
    "# # Note the funny way in which we have to join dfs in pandas :/\n",
    "candidate_labeled_tweets = raw_crowd_answers.join(gold_answers.set_index('tweet_id',drop=False),on=['tweet_id'],lsuffix='.raw',rsuffix='.gold',how='inner')\n",
    "candidate_labeled_tweets = candidate_labeled_tweets[['tweet_id.raw','tweet_body.raw','worker_id','emotion']]\n",
    "candidate_labeled_tweets.columns = ['tweet_id','tweet_body','worker_id','emotion']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, contributors can provide conflicting labels for the same tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1527     False\n",
       "1512     False\n",
       "1517     False\n",
       "1546     False\n",
       "1526     False\n",
       "1543     False\n",
       "1520     False\n",
       "1515     False\n",
       "1544     False\n",
       "1516     False\n",
       "1534     False\n",
       "1539     False\n",
       "1525     False\n",
       "1507     False\n",
       "1528     False\n",
       "1533     False\n",
       "1518     False\n",
       "1509     False\n",
       "1523     False\n",
       "1521     False\n",
       "1513     False\n",
       "1542     False\n",
       "1524     False\n",
       "1508     False\n",
       "1536     False\n",
       "1540     False\n",
       "1514     False\n",
       "7779     False\n",
       "7783     False\n",
       "7774     False\n",
       "         ...  \n",
       "14975    False\n",
       "15063    False\n",
       "15060    False\n",
       "14958    False\n",
       "14973    False\n",
       "14914    False\n",
       "15082    False\n",
       "14935    False\n",
       "14966    False\n",
       "14990    False\n",
       "15007    False\n",
       "15066    False\n",
       "15078    False\n",
       "15020    False\n",
       "14994    False\n",
       "14950    False\n",
       "14976    False\n",
       "15065    False\n",
       "14940    False\n",
       "14938    False\n",
       "14971    False\n",
       "15043    False\n",
       "15057    False\n",
       "14923    False\n",
       "15024    False\n",
       "14912    False\n",
       "15054    False\n",
       "15009    False\n",
       "15027    False\n",
       "15006    False\n",
       "Name: tweet_id, Length: 12640, dtype: bool"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_labeled_tweets.sort_values(['worker_id','tweet_id']).tweet_id == 79185673"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generating Snorkel Objects\n",
    "\n",
    "### `Candidates`\n",
    "\n",
    "`Candidates` are the core objects in Snorkel representing objects to be classified. We'll use a helper function to create a custom `Candidate` sub-class, `Tweet`, with values representing the possible labels that it can be classified with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "values = list(candidate_labeled_tweets.emotion.unique())\n",
    "\n",
    "Tweet = candidate_subclass('Tweet', ['tweet'], values=values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Contexts`\n",
    "\n",
    "All `Candidate` objects point to one or more `Context` objects, which represent the raw data that they are rooted in. In this case, our candidates will each point to a single `Context` object representing the raw text of the tweet.\n",
    "\n",
    "Once we have defined the `Context` for each `Candidate`, we can commit them to the database. Note that we also split into two sets while doing this:\n",
    "\n",
    "1. **Training set (`split=0`):** The tweets for which we have noisy, conflicting crowd labels; we will resolve these conflicts using the `GenerativeModel` and then use them as training data for the LSTM\n",
    "\n",
    "2. **Test set (`split=1`):** We will pretend that we do not have any crowd labels for this split of the data, and use these to test the LSTM's performance on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.models import Context, Candidate\n",
    "from snorkel.contrib.models.text import RawText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "632"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure DB is cleared\n",
    "session.query(Context).delete()\n",
    "session.query(Candidate).delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create the candidates with a simple loop\n",
    "tweet_bodies = candidate_labeled_tweets \\\n",
    "    [[\"tweet_id\", \"tweet_body\"]] \\\n",
    "    .sort_values(\"tweet_id\") \\\n",
    "    .drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and store the tweet candidates to be classified\n",
    "# Note: We split the tweets in two sets: one for which the crowd \n",
    "# labels are not available to Snorkel (test, 10%) and one for which we assume\n",
    "# crowd labels are obtained (to be used for training, 90%)\n",
    "total_tweets = len(tweet_bodies)\n",
    "tweet_list = []\n",
    "test_split = total_tweets*0.1\n",
    "for i, t in tweet_bodies.iterrows():\n",
    "    split = 1 if i <= test_split else 0\n",
    "    raw_text = RawText(stable_id=t.tweet_id, name=t.tweet_id, text=t.tweet_body)\n",
    "    tweet = Tweet(tweet=raw_text, split=split)\n",
    "    tweet_list.append(tweet)\n",
    "    session.add(tweet)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Labels`\n",
    "\n",
    "Next, we'll store the labels for each of the training candidates in a sparse matrix (which will also automatically be saved to the Snorkel database), with one row for each candidate and one column for each crowd worker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.annotations import LabelAnnotator\n",
    "from collections import defaultdict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A defaultdict works exactly like a normal dict, but it is initialized with a function (“default factory”)\n",
    "# that takes no arguments and provides the default value for a nonexistent key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract worker votes\n",
    "# Cache locally to speed up for this small set\n",
    "worker_labels = candidate_labeled_tweets[[\"tweet_id\", \"worker_id\", \"emotion\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12640, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worker_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wls = defaultdict(list)\n",
    "for i, row in worker_labels.iterrows():\n",
    "    wls[str(row.tweet_id)].append((str(row.worker_id), row.emotion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('18034918', 'Neutral / author is just sharing information'),\n",
       " ('18465660', 'Neutral / author is just sharing information'),\n",
       " ('18927389', 'Neutral / author is just sharing information'),\n",
       " ('17475684', 'Neutral / author is just sharing information'),\n",
       " ('14472526', 'Neutral / author is just sharing information'),\n",
       " ('18806438', 'Neutral / author is just sharing information'),\n",
       " ('14584835', 'Neutral / author is just sharing information'),\n",
       " ('14400603', 'Neutral / author is just sharing information'),\n",
       " ('12063015', 'Tweet not related to weather condition'),\n",
       " ('19028457', 'Positive'),\n",
       " ('14466721', 'Neutral / author is just sharing information'),\n",
       " ('15847995', 'Neutral / author is just sharing information'),\n",
       " ('10197897', 'Neutral / author is just sharing information'),\n",
       " ('20043586', 'Neutral / author is just sharing information'),\n",
       " ('7325249', 'Negative'),\n",
       " ('17948184', 'Neutral / author is just sharing information'),\n",
       " ('18500901', \"I can't tell\"),\n",
       " ('11800825', 'Neutral / author is just sharing information'),\n",
       " ('15124755', 'Neutral / author is just sharing information'),\n",
       " ('19955339', 'Neutral / author is just sharing information')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wls['82846118']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label generator\n",
    "def worker_label_generator(t):\n",
    "    \"\"\"A generator over the different (worker_id, label_id) pairs for a Tweet.\"\"\"\n",
    "    for worker_id, label in wls[t.tweet.name]:\n",
    "        yield worker_id, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 629/629 [00:02<00:00, 247.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.61 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<629x102 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 12580 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeler = LabelAnnotator(label_generator=worker_label_generator)\n",
    "%time L_train = labeler.apply(split=0)\n",
    "L_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we load the ground truth (\"gold\") labels for both the training and test sets, and store as numpy arrays\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_labels = defaultdict(list)\n",
    "\n",
    "# Get gold labels in verbose form\n",
    "verbose_labels = dict([(str(t.tweet_id), t.sentiment) \n",
    "                       for i, t in gold_answers[[\"tweet_id\", \"sentiment\"]].iterrows()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over splits, align with Candidate ordering\n",
    "\n",
    "for split in range(2):\n",
    "    cands = session.query(Tweet).filter(Tweet.split == split).order_by(Tweet.id).all() \n",
    "    for c in cands:\n",
    "        # Think this is just an odd way of label encoding between 1 and 5?\n",
    "        gold_labels[split].append(values.index(verbose_labels[c.tweet.name]) + 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cand_labels = np.array(gold_labels[0])\n",
    "test_cand_labels = np.array(gold_labels[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Resolving Crowd Conflicts with the Generative Model\n",
    "\n",
    "Until now we have converted the raw crowdsourced data into a labeling matrix that can be provided as input to `Snorkel`. We will now show how to:\n",
    "\n",
    "1. Use `Snorkel's` generative model to learn the accuracy of each crowd contributor.\n",
    "2. Use the learned model to estimate a marginal distribution over the domain of possible labels for each task.\n",
    "3. Use the estimated marginal distribution to obtain the maximum a posteriori probability estimate for the label that each task takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from snorkel.learning.gen_learning import GenerativeModel\n",
    "\n",
    "# Initialize Snorkel's generative model for\n",
    "# learning the different worker accuracies.\n",
    "gen_model = GenerativeModel(lf_propensity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred cardinality: 5\n"
     ]
    }
   ],
   "source": [
    "# Train the generative model\n",
    "gen_model.train(\n",
    "    L_train,\n",
    "    reg_type=2,\n",
    "    reg_param=0.1,\n",
    "    epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infering the MAP assignment for each task\n",
    "Each task corresponds to an independent random variable. Thus, we can simply associate each task with the most probably label based on the estimated marginal distribution and get an accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9952305246422893"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_model.score(L_train, train_cand_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9952305246422893\n",
      "Number incorrect:3\n"
     ]
    }
   ],
   "source": [
    "correct, incorrect = gen_model.error_analysis(session, L_train, train_cand_labels)\n",
    "print(\"Number incorrect:{}\".format(len(incorrect)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Majority vote\n",
    "\n",
    "It seems like we did well- but how well?  Given that this is a fairly simple task--we have 20 contributors per tweet (and most of them are far better than random)--**we expect majority voting to perform extremely well**, so we can check against majority vote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9841017488076311\n",
      "Number incorrect:10\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Collect the majority vote answer for each tweet\n",
    "mv = []\n",
    "for i in range(L_train.shape[0]):\n",
    "    c = Counter([L_train[i,j] for j in L_train[i].nonzero()[1]])\n",
    "    mv.append(c.most_common(1)[0][0])\n",
    "mv = np.array(mv)\n",
    "\n",
    "# Count the number correct by majority vote\n",
    "n_correct = np.sum([1 for i in range(L_train.shape[0]) if mv[i] == train_cand_labels[i]])\n",
    "print (\"Accuracy:{}\".format(n_correct / float(L_train.shape[0])))\n",
    "print (\"Number incorrect:{}\".format(L_train.shape[0] - n_correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that while majority vote makes 10 errors, the Snorkel model makes only 3!  What about an average crowd worker?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average human accuracy\n",
    "\n",
    "We see that the average accuracy of a single crowd worker is in fact much lower:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy:0.729664764868133\n"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "for j in range(L_train.shape[1]):\n",
    "    n_correct = np.sum([1 for i in range(L_train.shape[0]) if L_train[i,j] == train_cand_labels[i]])\n",
    "    acc = n_correct / float(L_train[:,j].nnz)\n",
    "    accs.append(acc)\n",
    "print( \"Mean Accuracy:{}\".format( np.mean(accs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training an ML Model with Snorkel for Sentiment Analysis over Unseen Tweets\n",
    "\n",
    "In the previous step, we saw that Snorkel's generative model can help to denoise crowd labels automatically. However, what happens when we don't have noisy crowd labels for a tweet?\n",
    "\n",
    "In this step, we'll use the estimates of the generative model as _probabilistic training labels_ to train a simple LSTM sentiment analysis model, which takes as input a tweet **for which no crowd labels are available** and predicts its sentiment.\n",
    "\n",
    "First, we get the probabilistic training labels (_training marginals_) which are just the marginal estimates of the generative model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_marginals = gen_model.marginals(L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 629 marginals\n"
     ]
    }
   ],
   "source": [
    "from snorkel.annotations import save_marginals\n",
    "save_marginals(session, L_train, train_marginals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll train a simple LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.learning.tensorflow import TextRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thomas.merritt-smith\\AppData\\Local\\conda\\conda\\envs\\snorkel\\lib\\site-packages\\snorkel\\learning\\tensorflow\\rnn\\rnn_base.py:36: UserWarning: Candidate 618 has argument past max length for model:\t[arg ends at index 28; max len 28]\n",
      "  warnings.warn('\\t'.join([w.format(i), info]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\thomas.merritt-smith\\AppData\\Local\\conda\\conda\\envs\\snorkel\\lib\\site-packages\\snorkel\\learning\\tensorflow\\rnn\\rnn_base.py:88: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "WARNING:tensorflow:From C:\\Users\\thomas.merritt-smith\\AppData\\Local\\conda\\conda\\envs\\snorkel\\lib\\site-packages\\snorkel\\learning\\tensorflow\\noise_aware_model.py:77: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thomas.merritt-smith\\AppData\\Local\\conda\\conda\\envs\\snorkel\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextRNN] Training model\n",
      "[TextRNN] n_train=629  #epochs=200  batch size=256\n",
      "[TextRNN] Epoch 0 (1.82s)\tAverage loss=1.547467\n",
      "[TextRNN] Epoch 5 (2.70s)\tAverage loss=0.177643\n",
      "[TextRNN] Epoch 10 (3.58s)\tAverage loss=0.050922\n",
      "[TextRNN] Epoch 15 (4.47s)\tAverage loss=0.054936\n",
      "[TextRNN] Epoch 20 (5.35s)\tAverage loss=0.026458\n",
      "[TextRNN] Epoch 25 (6.23s)\tAverage loss=0.024820\n",
      "[TextRNN] Epoch 30 (7.13s)\tAverage loss=0.024731\n",
      "[TextRNN] Epoch 35 (8.00s)\tAverage loss=0.027555\n",
      "[TextRNN] Epoch 40 (8.88s)\tAverage loss=0.024720\n",
      "[TextRNN] Epoch 45 (9.74s)\tAverage loss=0.029702\n",
      "[TextRNN] Epoch 50 (10.62s)\tAverage loss=0.022299\n",
      "[TextRNN] Epoch 55 (11.47s)\tAverage loss=0.023716\n",
      "[TextRNN] Epoch 60 (12.36s)\tAverage loss=0.020875\n",
      "[TextRNN] Epoch 65 (13.26s)\tAverage loss=0.027355\n",
      "[TextRNN] Epoch 70 (14.14s)\tAverage loss=0.023160\n",
      "[TextRNN] Epoch 75 (15.04s)\tAverage loss=0.023468\n",
      "[TextRNN] Epoch 80 (15.92s)\tAverage loss=0.020987\n",
      "[TextRNN] Epoch 85 (16.82s)\tAverage loss=0.019817\n",
      "[TextRNN] Epoch 90 (17.70s)\tAverage loss=0.022953\n",
      "[TextRNN] Epoch 95 (18.56s)\tAverage loss=0.018766\n",
      "[TextRNN] Epoch 100 (19.43s)\tAverage loss=0.020049\n",
      "[TextRNN] Epoch 105 (20.32s)\tAverage loss=0.018185\n",
      "[TextRNN] Epoch 110 (21.20s)\tAverage loss=0.022402\n",
      "[TextRNN] Epoch 115 (22.08s)\tAverage loss=0.019255\n",
      "[TextRNN] Epoch 120 (22.98s)\tAverage loss=0.020783\n",
      "[TextRNN] Epoch 125 (23.87s)\tAverage loss=0.017833\n",
      "[TextRNN] Epoch 130 (24.73s)\tAverage loss=0.018770\n",
      "[TextRNN] Epoch 135 (25.61s)\tAverage loss=0.019687\n",
      "[TextRNN] Epoch 140 (26.51s)\tAverage loss=0.019173\n",
      "[TextRNN] Epoch 145 (27.39s)\tAverage loss=0.018262\n",
      "[TextRNN] Epoch 150 (28.28s)\tAverage loss=0.019318\n",
      "[TextRNN] Epoch 155 (29.15s)\tAverage loss=0.017736\n",
      "[TextRNN] Epoch 160 (30.04s)\tAverage loss=0.019926\n",
      "[TextRNN] Epoch 165 (30.94s)\tAverage loss=0.019127\n",
      "[TextRNN] Epoch 170 (31.79s)\tAverage loss=0.018040\n",
      "[TextRNN] Epoch 175 (32.69s)\tAverage loss=0.016976\n",
      "[TextRNN] Epoch 180 (33.56s)\tAverage loss=0.020421\n",
      "[TextRNN] Epoch 185 (34.44s)\tAverage loss=0.020856\n",
      "[TextRNN] Epoch 190 (35.33s)\tAverage loss=0.019307\n",
      "[TextRNN] Epoch 195 (36.25s)\tAverage loss=0.020573\n",
      "[TextRNN] Epoch 199 (36.94s)\tAverage loss=0.015708\n",
      "[TextRNN] Training done (36.94s)\n"
     ]
    }
   ],
   "source": [
    "train_kwargs = {\n",
    "    'lr':         0.01,\n",
    "    'dim':        100,\n",
    "    'n_epochs':   200,\n",
    "    'dropout':    0.2,\n",
    "    'print_freq': 5\n",
    "}\n",
    "\n",
    "lstm = TextRNN(seed=1701, cardinality=Tweet.cardinality)\n",
    "train_cands = session.query(Tweet).filter(Tweet.split == 0).order_by(Tweet.id).all()\n",
    "lstm.train(train_cands, train_marginals, **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cands = session.query(Tweet).filter(Tweet.split == 1).order_by(Tweet.id).all()\n",
    "lstm.score(test_cands, test_cand_labels)\n",
    "# print (\"Number incorrect:{}\".format(len(incorrect)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we're already close to the accuracy of an average crowd worker! If we wanted to improve the score, we could tune the LSTM model using grid search (see the Intro tutorial), use [pre-trained word embeddings](https://nlp.stanford.edu/projects/glove/), or many other common techniques for getting state-of-the-art scores. Notably, we're doing this without using gold labels, but rather noisy crowd-labels!\n",
    "\n",
    "For more, checkout the other tutorials!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (snorkel)",
   "language": "python",
   "name": "snorkel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
